---
title: "C11 Computing Notes"
output: html_document
---

```{r, message=FALSE}
library("AppliedPredictiveModeling")
library("caret"); library("klaR"); library("MASS")
library("pROC"); library("randomForest")
library("doMC")
registerDoMC(cores = 4)
```

```{r}
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
```

Lets use a random forest and quadratic discriminant model to fit the data

```{r, cache=TRUE}
# mtry will take the default
# for classification is sqrt(p), for regression in p/3
# where p is number of predictors
rfModel <- randomForest(class ~ X1 + X2,
                       data = simulatedTrain,
                       ntree = 2000)

qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)
```

For QDA, the output of the preict function for qda objects includes both the predicted classes (in a slot called class), and the associated probabilities are in a matrix called posterior

```{r}
# get predictions on training and test samples
qdaTrainPred <- predict(qdaModel, simulatedTrain)
qdaTestPred <- predict(qdaModel, simulatedTest)

# have a quick nose
names(qdaTrainPred)
head(qdaTrainPred$class)
head(qdaTrainPred$posterior)

# extract probabilities
simulatedTrain$QDAprob <- qdaTrainPred$posterior[, "Class1"]
simulatedTest$QDAprob <- qdaTestPred$posterior[, "Class1"]
```

The random forest model requires two calls to the predict function to get the predicted class and class probabilities:

```{r}
# just for the test set here
rfTestPred <- predict(rfModel, simulatedTest, type = "prob")
head(rfTestPred)

simulatedTest$RFprob <- rfTestPred[, "Class1"]
simulatedTest$RFclass <- predict(rfModel, simulatedTest)
```

## Sensitivity and Specificity

the caret package has function for sensitivity and specificity. These functions require the user to indicate the role of interest in each of the classes:

```{r}
# Class 1 is class of interest
sensitivity(data = simulatedTest$RFclass,
            reference = simulatedTest$class,
            positive = "Class1")

specificity(data = simulatedTest$RFclass,
            reference = simulatedTest$class,
            positive = "Class1")
```

Predicctive values can also be computed either using the prevalence in the data set or by using prior judgement:

```{r}
# prevalence in data set is 46%
sum(simulatedTest$class == "Class1") / nrow(simulatedTest)

posPredValue(data = simulatedTest$RFclass,
             reference = simulatedTest$class,
             positive = "Class1")

negPredValue(data = simulatedTest$RFclass,
             reference = simulatedTest$class,
             positive = "Class2")

# change the prevalence manually
posPredValue(data = simulatedTest$RFclass,
             reference = simulatedTest$class,
             positive = "Class1",
             prevalence = 0.9)
```

## Confusion Matrix

The confusionMatrix function in the caret package produces the tavle and associated statistics:

```{r}
confusionMatrix(data = simulatedTest$RFclass,
                reference = simulatedTest$class,
                positive = "Class1")
```

There is an option in confusionMatrix to manually set the prevalence. If there are more than two classes, the sensitivity, specififity and similar statitsics are calalculated on a one-vs-all basis (eg the first class versus a pool of classes two and three).

## ROC Curves

The pROC package can create the curve and deive various stattistics. First, an R object must be created that contains the relevant information using the pROC function roc()

```{r, fig.width=5, fig.height=5}
rocCurveRF <- roc(response = simulatedTest$class,
                predictor = simulatedTest$RFprob,
                # this function assumes that the second
                # class is the event of interest, so we
                # reverse the labels
                levels = rev(levels(simulatedTest$class)))

rocCurveQDA <- roc(response = simulatedTest$class,
                predictor = simulatedTest$QDAprob,
                # this function assumes that the second
                # class is the event of interest, so we
                # reverse the labels
                levels = rev(levels(simulatedTest$class)))

# can produce statistics
auc(rocCurveRF)
ci(rocCurveRF)

# by defailut the x-axis goes backwards
# use the legacy.axes = TRUE to get 1-spec on the x-axis
plot(rocCurveRF, legacy.axes = TRUE)

# can also add another curve by using add = TRUE next time plot.auc is used
plot(rocCurveQDA, legacy.axes = TRUE, add = T)
```

## Lift Charts

The lift curve can be created using the lift functoin in the caret package. It takes a formula as the input where the true class is on the LHS, and one or more columns for the model class probabilities on the right

```{r, fig.width=5, fig.height=5}
labs <- c(RFprob = "Random Forest",
          QDAprob = "QDA")
liftCurve <- lift(class ~ RFprob + QDAprob, data = simulatedTest,
                  labels = labs)
liftCurve

xyplot(liftCurve,
       auto.key = list(columns = 2,
                       lines = T,
                       points = F))
```

## Calibrating Probabilities

Calibrartion plots can be made using the caret function calibration()

```{r}
calCurve <- calibration(class ~ RFprob + QDAprob, data = simulatedTest)
calCurve

xyplot(calCurve,
       auto.key = list(columns = 2))
```

From the calibration plot, we see that we may want to recalibrate (especially for QDA). To fit a sigmoidal function, a logistic regression model can be used.

```{r}
# glm function from base R. Select familiy = binary as two class problem
sigmoidalCal <- glm(relevel(class, ref = "Class2") ~ QDAprob,
                    data = simulatedTrain,
                    family = binomial)
coef(summary(sigmoidalCal))
```

We can then obtain the corrected probailities by taking the origional model and applying the correction with the estimated slope and intercept (see notes)

$$\hat{p}^\ast = \frac{1}{1 + \exp(-\beta_0 - \beta_1 \hat{p})} $$

```{r}
sigmoidProbs <- predict(sigmoidalCal,
                        newdata = simulatedTest[, "QDAprob", drop = F],
                        type = "response")

simulatedTest$QDAsigmoid <- sigmoidProbs
```


The Bayesian approach can be used as well (will learn about this in later chapters). Can use the NaiveBayes in the klaR package:

```{r}
# the option usekernel = TRUE allows a flexible function to model
# the probability distribution of the class probs
BayesCal <- NaiveBayes(class ~ QDAprob,
                       data = simulatedTrain,
                       usekernel = TRUE)
# Like qda() the predict function for this model creates both
# the classes and probabilities

BayesProbs <- predict(BayesCal,
                      newdata = simulatedTest[, "QDAprob", drop = F])
simulatedTest$QDABayes <- BayesProbs$posterior[, "Class1"]

# look before and after calibration
head(simulatedTest[, c(5:6, 8, 9)])

# another plot:
calCurve2 <- calibration(class ~ QDAprob + QDABayes + QDAsigmoid,
                         data = simulatedTest)
xyplot(calCurve2,
       auto.key = list(columns = 3))
```