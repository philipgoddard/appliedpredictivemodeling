rm(list = ls())
library("caret")
library("AppliedPredictiveModeling")
library("glmnet")
library("MASS")
library("pamr")
library("pls")
library("pROC")
library("rms")
library("sparseLDA")
library("subselect")
library("RColorBrewer")
library("devtools")
library("ggbiplot")
library("reshape2")
library("doMC")
library("C50")
library("dplyr")
library("klaR")
registerDoMC(4)
data(churn)
#View(churnTrain)
summary(churnTrain)
# Maximise ROC, or kappa if poor (as strong class imbalance)
trainOutcome <- churnTrain$churn
testOutcome <- churnTest$churn
trainPred <- churnTrain[!names(churnTrain) == "churn"]
testPred <- churnTest[!names(churnTest) == "churn"]
# seperate out factor, double (continuous) and integer (count) predictors
vapply(churnTrain, class, character(1))
facCols <- trainPred[, vapply(trainPred, is.factor, logical(1))]
numCols <- trainPred[, vapply(trainPred, is.double, logical(1))]
intCols <- trainPred[, vapply(trainPred, is.integer, logical(1))]
# test
facColsT <- testPred[, vapply(testPred, is.factor, logical(1))]
numColsT <- testPred[, vapply(testPred, is.double, logical(1))]
intColsT <- testPred[, vapply(testPred, is.integer, logical(1))]
##################################################
# step 2: seperate continuous predictictors, create interaction terms
# plot vs outcome to see if any worth retaining
quadInts <- quadInteraction(numCols)
quadIntsT <- quadInteraction(numColsT)
trans <- preProcess(quadInts, method = "BoxCox")
quadIntTrans <- predict(trans, quadInts)
quadIntTransT <- predict(trans, quadIntsT)
#####################################################
multiplotList <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
plots <-  c(..., plotlist)
numPlots = length(plots)
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
########################################################
quadInteraction <- function(df){
stopifnot(is.data.frame(df))
numericCols <- vapply(df, is.numeric, logical(1))
dfNum <- df[, numericCols]
nCols <- ncol(df) + choose(ncol(dfNum), 2) + ncol(dfNum)
out <- data.frame(matrix(NA, nrow = nrow(df), ncol = nCols))
out[1:ncol(df)] <- df
names(out)[1:ncol(df)] = names(df)
start <- 1
dfPosition <- ncol(df) + 1
for( i in 1:ncol(dfNum) ) {
for( j in start:ncol(dfNum) ) {
out[dfPosition] <- dfNum[i] * dfNum[j]
names(out)[dfPosition] <- paste(names(dfNum[i]),
names(dfNum[j]),
sep = ' * ')
dfPosition <- dfPosition + 1
}
start <- start + 1
}
out
}
#########################################################
replace <- function(x, curr, repl, regex = FALSE, ...) {
stopifnot(length(curr) == length(repl))
stopifnot(is.character(curr) && is.character(repl))
for(i in 1:length(curr)){
# if(regex){
x <- gsub(curr[i], repl[i], x, ...)
#  } else {
#    x[x == curr[i] ] <- repl[i]
#  }
}
x
}
############################################################
mround <- function(x, base){
base * round(x / base)
}
quadInts <- quadInteraction(numCols)
quadIntsT <- quadInteraction(numColsT)
trans <- preProcess(quadInts, method = "BoxCox")
quadIntTrans <- predict(trans, quadInts)
quadIntTransT <- predict(trans, quadIntsT)
catDummies <- dummyVars(~. ,
data = facCols)
facTrans <- data.frame(predict(catDummies, facCols))
facTransT <- data.frame(predict(catDummies, facColsT))
# step 3a: remove any ZV predictors (shouldnt be any)
nzvFac <- nearZeroVar(facTrans, saveMetric = TRUE)
length(nzvFac[, nzvFac$zeroVar == TRUE])
# careful- the states have very high freq ratios... (all near zero var...)
# consider removing for reduced set
# step 3b: remove collinear (and rename)
reducedCovMat <- cov(facTrans[, 52:58])
trimmingResults <- trim.matrix(reducedCovMat)
trimmingResults$names.discarded
# remove and rename
facTrans <- facTrans %>%
select(1:53, international_plan.no, voice_mail_plan.yes ) %>%
rename(voice_mail_plan = voice_mail_plan.yes,
international_plan = international_plan.no)
facTransT <- facTransT %>%
select(1:53, international_plan.no, voice_mail_plan.yes ) %>%
rename(voice_mail_plan = voice_mail_plan.yes,
international_plan = international_plan.no)
intColsTrans <- intCols %>%
mutate(number_vmail_messages = ifelse(number_vmail_messages == 0, 0, 1),
number_customer_service_calls = sqrt(number_customer_service_calls),
total_intl_calls = log(total_intl_calls + 1))
intColsTransT <- intColsT %>%
mutate(number_vmail_messages = ifelse(number_vmail_messages == 0, 0, 1),
number_customer_service_calls = sqrt(number_customer_service_calls),
total_intl_calls = log(total_intl_calls + 1))
trainInput <- cbind(quadIntTrans, facTrans, intColsTrans)
testInput <- cbind(quadIntTransT, facTransT, intColsTransT)
# step 5b: same processing for test set trainInputT
# step 6: make full and reduced set
# full set: remove ZV
# reduced set: remove nzv
isNZV <- nearZeroVar(trainInput, saveMetrics = TRUE)
fullSet <- names(trainInput[, !isNZV$zeroVar])
reducedSet <- names(trainInput[, !isNZV$nzv])
trainCorr <- cor(trainInput)
highCorr <- findCorrelation(trainCorr, cutoff = 0.9)
fullCorr <- findCorrelation(trainCorr, cutoff = 0.99)
highCorrNames <- names(trainInput)[highCorr]
fullCorrNames <- names(trainInput)[fullCorr]
fullSet <- fullSet[!fullSet %in% fullCorrNames]
reducedSet <- reducedSet[!reducedSet %in% highCorrNames]
reducedCovMat <- cov(trainInput[, reducedSet])
trimmingResults <- trim.matrix(reducedCovMat)
trimmingResults$names.discarded
trainPCA <- prcomp(trainInput[, reducedSet], scale=TRUE)
type <- trainOutcome
reduced_pca <- ggbiplot(trainPCA, obs.scale = 1,
var.scale = 1,
groups = type,
ellipse = TRUE,
circle = TRUE,
var.axes = FALSE,
varname.size = 3)
reduced_pca + theme_bw() + scale_colour_brewer(palette = "Set1")
ctrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
classProbs = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary)
set.seed(202)
# estimate sigma
sigmaRangeReduced <- sigest(as.matrix(trainInput[, reducedSet]))
svmRGridReduced <- expand.grid(sigma = sigmaRangeReduced[1],
C = 2^(seq(-4, 4)))
set.seed(476)
svmRTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "svmRadial",
metric = "Sens",
preProc = c("center", "scale"),
tuneGrid = svmRGridReduced,
fit = FALSE,
trControl = ctrl)
set.seed(202)
# estimate sigma
sigmaRangeReduced <- sigest(as.matrix(trainInput[, reducedSet]))
svmRGridReduced <- expand.grid(sigma = sigmaRangeReduced[1],
C = 2^(seq(-4, 4)))
set.seed(476)
svmRTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "svmRadial",
metric = "Sens",
preProc = c("center", "scale"),
tuneGrid = svmRGridReduced,
fit = FALSE,
trControl = ctrl)
svmRTune
rm(list = ls())
library("caret")
library("AppliedPredictiveModeling")
library("nnet")
library("kernlab")
library("MASS")
library("mda")
library("earth")
library("klaR")
library("rrcov")
library("pROC")
library("rms")
library("sparseLDA")
library("subselect")
library("RColorBrewer")
library("devtools")
library("ggbiplot")
library("reshape2")
library("doMC")
library("C50")
library("dplyr")
registerDoMC(4)
data(churn)
#View(churnTrain)
summary(churnTrain)
# Maximise ROC, or kappa if poor (as strong class imbalance)
trainOutcome <- churnTrain$churn
testOutcome <- churnTest$churn
trainPred <- churnTrain[!names(churnTrain) == "churn"]
testPred <- churnTest[!names(churnTest) == "churn"]
# what is the no information rate in the test set?
##################################################
# step 1: some visualistaions
# seperate out factor, double (continuous) and integer (count) predictors
vapply(churnTrain, class, character(1))
facCols <- trainPred[, vapply(trainPred, is.factor, logical(1))]
numCols <- trainPred[, vapply(trainPred, is.double, logical(1))]
intCols <- trainPred[, vapply(trainPred, is.integer, logical(1))]
# test
facColsT <- testPred[, vapply(testPred, is.factor, logical(1))]
numColsT <- testPred[, vapply(testPred, is.double, logical(1))]
intColsT <- testPred[, vapply(testPred, is.integer, logical(1))]
# step 3: categorical predictors to dummy
catDummies <- dummyVars(~. ,
data = facCols)
facTrans <- data.frame(predict(catDummies, facCols))
facTransT <- data.frame(predict(catDummies, facColsT))
# step 3a: remove any ZV predictors (shouldnt be any)
nzvFac <- nearZeroVar(facTrans, saveMetric = TRUE)
length(nzvFac[, nzvFac$zeroVar == TRUE])
reducedCovMat <- cov(facTrans[, 52:58])
trimmingResults <- trim.matrix(reducedCovMat)
trimmingResults$names.discarded
# remove and rename
facTrans <- facTrans %>%
select(1:53, international_plan.no, voice_mail_plan.yes ) %>%
rename(voice_mail_plan = voice_mail_plan.yes,
international_plan = international_plan.no)
facTransT <- facTransT %>%
select(1:53, international_plan.no, voice_mail_plan.yes ) %>%
rename(voice_mail_plan = voice_mail_plan.yes,
international_plan = international_plan.no)
##################################################
# step 5: combine predictors
# do not do transformed at this stage
# not thet factTrans is just transformed to dummy
trainInput <- cbind(numCols, facTrans, intCols)
testInput <- cbind(numColsT, facTransT, intColsT)
# step 5b: same processing for test set trainInputT
# step 6: make full and reduced set
# full set: remove ZV
# reduced set: remove nzv
# NOTE trees will probably be immune to nzv predictors in state
# but for several nonlinear models (eg nnet and svm)
# uninformative predictors will upset
isNZV <- nearZeroVar(trainInput, saveMetrics = TRUE)
fullSet <- names(trainInput[, !isNZV$zeroVar])
reducedSet <- names(trainInput[, !isNZV$nzv])
# full set: correlation threshold = 0.99
# reduced set: correlation threshold = 0.9
# using the transformed intCols upsets this - WHY??
trainCorr <- cor(trainInput)
highCorr <- findCorrelation(trainCorr, cutoff = 0.9)
fullCorr <- findCorrelation(trainCorr, cutoff = 0.99)
highCorrNames <- names(trainInput)[highCorr]
fullCorrNames <- names(trainInput)[fullCorr]
fullSet <- fullSet[!fullSet %in% fullCorrNames]
reducedSet <- reducedSet[!reducedSet %in% highCorrNames]
# check for colinearity (shouldnt be any as already checked
# when made factors)
# test for collinearity (for reduced)
# once again some nonlinear models will be
# adversely effected
reducedCovMat <- cov(trainInput[, reducedSet])
trimmingResults <- trim.matrix(reducedCovMat)
trimmingResults$names.discarded
trainPCA <- prcomp(trainInput[, reducedSet], scale=TRUE)
type <- trainOutcome
reduced_pca <- ggbiplot(trainPCA, obs.scale = 1,
var.scale = 1,
groups = type,
ellipse = TRUE,
circle = TRUE,
var.axes = FALSE,
varname.size = 3)
reduced_pca + theme_bw() + scale_colour_brewer(palette = "Set1")
View(trainInput[, reducedSet])
# step 7: fit models
# 7a: train control
ctrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
classProbs = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary)
set.seed(202)
# estimate sigma
sigmaRangeReduced <- sigest(as.matrix(trainInput[, reducedSet]))
svmRGridReduced <- expand.grid(sigma = sigmaRangeReduced[1],
C = 2^(seq(-4, 4)))
sigmaRangeReduced
sigmaRangeReduced[1]
set.seed(476)
sigmaRangeReduced <- sigest(as.matrix(trainInput[, reducedSet]))
sigmaRangeReduced[1]
ctrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
classProbs = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary)
# tune over lambda and gamma
set.seed(476)
qdaTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "rda",
preProc = c("center", "scale"),
metric = "Sens",
trControl = ctrl)
qdaTune
getwd()
setwd("~/R/appliedpredictivemodeling/C13")
save(qdaTune, "qdaChurn.RData")
qdaTune
?save
save(qdaTune, file ="qdaChurn.RData")
mdaGrid <- expand.grid(subclasses = 1:12)
set.seed(476)
mdaTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "mda",
preProc = c("center", "scale"),
metric = "Sens",
tuneGrid = mdaGrid,
trControl = ctrl)
mdaTune
save(mdaTune, file ="mdaChurn.RData")
nnetGrid <- expand.grid(size = 1:10,
decay = c(0, 0.1, 1, 2))
maxSize <- max(nnetGrid$size)
numWts <- 1*(maxSize * (length(reducedSet) + 1) + maxSize + 1)
set.seed(476)
nnetTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "nnet",
metric = "Sens",
preProc = c("center", "scale", "spatialSign"),
tuneGrid = nnetGrid,
trace = FALSE,
maxit = 1000,
MaxNWts = numWts,
trControl = ctrl)
nnetTune
save(nnetTune, file ="nnetChurn.RData")
plot(nnetTune)
nnetPred <- predict(nnetTune, newdata = testInput[, reducedSet])
confusionMatrix(data = nnetPred, reference = testOutcome)
# fda with MARS
marsGrid <- expand.grid(degree = 1,
nprune = seq(2, 40, 2))
set.seed(476)
fdaTune <- train(x = trainInput[, fullSet],
y = trainOutcome,
method = "fda",
metric = "Sens",
preProc = c("center", "scale"),
tuneGrid = marsGrid,
trControl = ctrl)
fdaTune
save(fdaTune, file ="fdaChurn.RData")
set.seed(202)
# estimate sigma
sigmaRangeReduced <- sigest(as.matrix(trainInput[, reducedSet]))
svmRGridReduced <- expand.grid(sigma = sigmaRangeReduced[1],
C = 2^(seq(-4, 4)))
set.seed(476)
svmRTune <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "svmRadial",
metric = "Sens",
preProc = c("center", "scale"),
tuneGrid = svmRGridReduced,
fit = FALSE,
trControl = ctrl)
svmRTune
svmPred <- predict(svmRTune, newdata = testInput[, reducedSet])
confusionMatrix(data = svmPred, reference = testOutcome)
save(svmRTune, file ="svmRChurn.RData")
# k-nn
set.seed(476)
knnFit <- train(x = trainInput[, reducedSet],
y = trainOutcome,
method = "knn",
metric = "Sens",
preProc = c("center", "scale"),
tuneGrid = data.frame(k = seq(3, 50, 2)),
trControl = ctrl)
knnFit
save(knnFit, file ="knnChurn.RData")
models <- list(rda = qdaTune,
mda = mdaTune,
nnet = nnetTune,
fda = fdaTune,
svm = svmRTune,
knn = knnFit)
inputSet <- list(rdaData = testInput[, reducedSet],
mdaData = testInput[, reducedSet],
nnetData = testInput[, reducedSet],
fdaData = testInput[, fullSet],
svmData = testInput[, reducedSet],
knnData = testInput[, reducedSet])
predictions <- Map(function(x, y) {
out <- list()
out[[1]] <- predict(x, newdata = y)
out[[2]] <- predict(x, newdata = y, type = "prob")$yes
out[[3]] <- confusionMatrix(data = predict(x, newdata = y), reference = testOutcome)
out
},
models,
inputSet)
results <- do.call(cbind.data.frame, lapply(predictions, "[[", 2))
results$class <- testOutcome
head(results)
calCurve <- calibration(class ~  mda , data = results)
calCurve
xyplot(calCurve,
auto.key = list(columns = 3))
calCurve <- calibration(class ~  svm + nnet , data = results)
calCurve
xyplot(calCurve,
auto.key = list(columns = 3))
liftCurve <- lift(class ~ svm + nnet, data = results)
liftCurve
xyplot(liftCurve,
auto.key = list(columns = 2,
lines = T,
points = F))
modelstoCal <- select(results, rda, mda, fda, class)
calibrationModels <- lapply(names(modelstoCal)[1:3],
function(colName){
model <- NaiveBayes(reformulate(termlabels = colName, response = 'class'),
data = modelstoCal,
usekernel = TRUE)})
calibrationResults <- Map(
function(x, y){
out <- list()
out[[1]] <- predict(x, newdata = as.data.frame(y))$posterior[, "yes"]
out[[2]] <- predict(x, newdata = as.data.frame(y))$class
out[[3]] <- confusionMatrix(data = predict(x, newdata = as.data.frame(y))$class,
reference = testOutcome)
out
},
calibrationModels,
modelstoCal[, 1:3]
)
calibrated <- do.call(cbind.data.frame, lapply(calibrationResults, '[[', 1))
names(calibrated) <- c("rdaCal", "mdaCal", "fdaCal")
calibrated$class <- testOutcome
calCurve <- calibration(class ~  rdaCal + mdaCal + fdaCal , data = calibrated)
calCurve
xyplot(calCurve,
auto.key = list(columns = 3))
head(results)
head(calibrationResults)[[2]]
head(calibrationResults)[1][[2]]
head(calibrationResults)[[[1]][[2]]
head(calibrationResults)[[1]][[2]]
head(calibrationResults)[[1]][[1]]
head(calibrationResults)[[1]][[3]]
head(calibrationResults)[[2]][[3]]
head(calibrationResults)[[3]][[3]]
